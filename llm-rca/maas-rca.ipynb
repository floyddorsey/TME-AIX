{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model As a Service with RCA after anomaly detection using RAG<br>\n",
    "## Project Overview<br>\n",
    "Author:  Sedat Kaymaz & Fatih E. NAR <br>\n",
    "Update: Adding support of Model as a Service BackEnd. Ref: https://maas.apps.prod.rhoai.rh-aiservices-bu.com/<br> \n",
    "This project aims to provide a root cause analsys method by using LLM with Dynamic RAG based on a system log file <br>\n",
    "for reference after applying anomaly detection ML method to the basic telecom metric list.   <br>\n",
    "Please NOTE: As MaaS does not offer proper embedding generation service (yet, our efforts ended with 500 server internal error so far) the correlation between metric anomaly and log datameshing can be poor. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once only\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    \"\"\"Retrieve the Language Model (LLM) based on the user's choice of OpenAI or server.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        model_source (str): 'openai' for OpenAI API, 'server' for MaaS Backend.\n",
    "        model_name (str): Model name, defaults to 'gpt-4' for OpenAI.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        ChatOpenAI or dict: LLM object for OpenAI or dictionary with server configuration.\n",
    "\n",
    "    \"\"\"\n",
    "    #If you are having issues with api key entry via embedded input, you can uncomment the line below and replace 'put_your_key_here' with your actual key\n",
    "    #os.environ[\"API_KEY\"] = 'put_your_key_here'\n",
    "    maas_api_key = os.getenv(\"API_KEY\")\n",
    "    if not maas_api_key:\n",
    "        maas_api_key = input(\"Please enter your MaaS API key: \").strip()\n",
    "        os.environ[\"API_KEY\"] = maas_api_key\n",
    "        print(\"MaaS API key has been set.\")\n",
    "    return {\"server_url\": maas_server_url, \"api_key\": maas_api_key}  # Dictionary format for server configuration\n",
    "\n",
    "MAAS_MAX_CONTEXT_LENGTH = 4000\n",
    "maas_server_url = 'https://mistral-7b-instruct-v0-3-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/chat/completions'\n",
    "llm = get_llm()\n",
    "API_KEY = os.getenv('API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  call_attempt  call_success  call_failure  \\\n",
      "0 2024-09-04 00:00:00           114           110             0   \n",
      "1 2024-09-04 00:01:00           113           110             0   \n",
      "2 2024-09-04 00:02:00           114           111             0   \n",
      "3 2024-09-04 00:03:00           113           111             1   \n",
      "4 2024-09-04 00:04:00           112           111             1   \n",
      "\n",
      "   total_registered_subs  call_success_rate  \n",
      "0                   9031              96.40  \n",
      "1                   9084              97.34  \n",
      "2                   9089              97.36  \n",
      "3                   9035              98.23  \n",
      "4                   9092              99.10  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process metrics file with minimal MaaS embedding update\n",
    "# Only applies to the embedding part to avoid OpenAI dependency for MaaS Backend\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    \"\"\"Custom embedding class for FAISS compatibility, with `embed_documents` and `embed_query` methods.\"\"\"\n",
    "    def embed_documents(self, texts):\n",
    "        # Example embeddings; replace with actual embeddings from the MaaS backend if available\n",
    "        return [np.random.rand(768) for _ in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return np.random.rand(768)  # Example embedding for a single query; replace if available\n",
    "\n",
    "\n",
    "def process_metrics_file(filename):\n",
    "    \"\"\"\n",
    "    Process the metrics file and return a vector store with embeddings based on model source.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the metrics file.\n",
    "        model_source (str): Model source, 'openai' for OpenAI API or 'server' for MaaS Backend.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore: A vector store containing the embeddings of the text documents.\n",
    "    \"\"\"\n",
    "    loader = CSVLoader(f\"data/{filename}\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = CustomEmbeddings()  # Use CustomEmbeddings for compatibility with FAISS\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "    \n",
    "# Process metrics file\n",
    "metrics_vectorstore = process_metrics_file(\"metrics.csv\")\n",
    "\n",
    "# Load metrics data for anomaly detection\n",
    "df = pd.read_csv(\"data/metrics.csv\")\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies found:\n",
      "                   time  call_attempt  call_success  call_failure  \\\n",
      "683 2024-09-04 11:23:00           114            27             0   \n",
      "684 2024-09-04 11:24:00           113            32             0   \n",
      "685 2024-09-04 11:25:00           112            40             0   \n",
      "686 2024-09-04 11:26:00           114            70             2   \n",
      "\n",
      "     total_registered_subs  call_success_rate  is_anomaly  \n",
      "683                   9029             23.491          -1  \n",
      "684                   9038             28.368          -1  \n",
      "685                   9033             35.730          -1  \n",
      "686                   9157             61.491          -1  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anomaly detection using Isolation Forest\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"\n",
    "    Detects anomalies in the given DataFrame using the Isolation Forest algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame containing the data to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A subset of the input DataFrame containing only the rows that are classified as anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    features = ['call_attempt', 'call_success', 'call_failure', 'total_registered_subs', 'call_success_rate']\n",
    "    X = df[features]\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.005, random_state=42)\n",
    "    anomalies = iso_forest.fit_predict(X)\n",
    "    \n",
    "    df['is_anomaly'] = anomalies\n",
    "    return df[df['is_anomaly'] == -1]\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = detect_anomalies(df)\n",
    "\n",
    "if anomalies.empty:\n",
    "    print(\"No anomalies detected in the metrics.\")\n",
    "\n",
    "print(f\"Anomalies found:\\n{anomalies}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing log file...\n",
      "Log file has been processed.\n"
     ]
    }
   ],
   "source": [
    "# RAG for processing log file\n",
    "def process_log_file(filename):\n",
    "    \"\"\"\n",
    "    Process a log file and return a vector store.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the log file to process.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore: A vector store containing embeddings of the log file texts.\n",
    "    \"\"\"\n",
    "    loader = TextLoader(f\"data/{filename}\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = CustomEmbeddings()  # Use CustomEmbeddings for compatibility with FAISS\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Process log file\n",
    "print(\"Processing log file...\")\n",
    "logs_vectorstore = process_log_file(\"systemd.log\")\n",
    "print(\"Log file has been processed.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Cause Analysis:\n",
      " To perform a root cause analysis on the provided anomalies, let's first understand the context and the metrics involved.\n",
      "\n",
      "1. Metrics:\n",
      "   - Time: The date and time when the metrics were collected.\n",
      "   - Call Attempt: The total number of calls made during the time period.\n",
      "   - Call Success: The number of calls that were successfully completed.\n",
      "   - Call Failure: The number of calls that failed during the time period.\n",
      "   - Total Registered Subs: The total number of subscribers registered in the system.\n",
      "   - Call Success Rate: The percentage of successful calls out of the total number of attempts.\n",
      "\n",
      "2. Anomalies:\n",
      "   - The anomalies data provides the time, call attempt, call success, call failure, total registered subs, and call success rate for four different time instances. The call success rate seems unusually high (98.24% and 98.23%) for the first two data points, but then it drastically increases (61.49%) for the fourth data point.\n",
      "\n",
      "3. Logs:\n",
      "   - The logs provided show the system boot up and start up of the OpenStack Compute Service (Nova Compute). There are no logs suggesting any errors or issues that could directly explain the sudden increase in call failures.\n",
      "\n",
      "Given the information available, it's difficult to pinpoint a precise root cause for the anomaly. However, the sudden increase in call failures (from 0 to 70) in a single data point (2024-09-04 11:26:00) and the drastic drop in call success rate (from 98.24% to 61.49%) could be indicative of a service degradation or an issue specific to the time the anomaly occurred.\n",
      "\n",
      "To further investigate this issue, consider the following steps:\n",
      "\n",
      "1. Check system and service logs during and around the time of the anomaly for any error messages or indications of unusual activity.\n",
      "2. Monitor the system's resource usage (CPU, memory, disk I/O) to ensure there were no resource shortages during the anomaly period that could have affected call handling.\n",
      "3. Investigate the network connectivity for any issues that could have caused call failures.\n",
      "4. Analyze the call details (e.g., call duration, call type, destination) during the anomaly to see if there's a pattern or common factor among the failed calls.\n",
      "5. If possible, gather feedback from users or subscribers during the anomaly to understand their experience and whether they noticed any issues while making calls.\n",
      "\n",
      "Based on the findings from these investigations, you can narrow down the possible root causes and implement appropriate solutions to address the issue.\n"
     ]
    }
   ],
   "source": [
    "def analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies):\n",
    "    # Prepare the anomalies list\n",
    "    anomalies_list = anomalies.astype(str).values.tolist() if isinstance(anomalies, pd.DataFrame) else anomalies\n",
    "\n",
    "    # Retrieve related metrics and logs information\n",
    "    metrics_embedding = metrics_vectorstore.embedding_function.embed_query(str(anomalies_list))\n",
    "    logs_embedding = logs_vectorstore.embedding_function.embed_query(str(anomalies_list))\n",
    "\n",
    "    metrics_info = [doc.page_content[:500] for doc in metrics_vectorstore.similarity_search_by_vector(metrics_embedding, k=2)]  # Truncate to limit token usage\n",
    "    logs_info = [doc.page_content[:500] for doc in logs_vectorstore.similarity_search_by_vector(logs_embedding, k=2)]  # Truncate to limit token usage\n",
    "\n",
    "    # Prepare headers and payload for the MaaS server\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': API_KEY,\n",
    "    }\n",
    "\n",
    "    # Prepare content with token count calculation\n",
    "    messages_content = f\"Anomalies: {anomalies_list}\\nMetrics Info: {metrics_info}\\nLogs Info: {logs_info}\"\n",
    "    message_token_count = len(messages_content.split())  # Rough token approximation based on word count\n",
    "    max_tokens = max(0, 6144 - message_token_count - 550)  # Buffer for completion tokens\n",
    "\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": \"You are a root cause analysis assistant. Provide a detailed analysis of anomalies using the provided metrics and logs.\",\n",
    "                \"role\": \"system\",\n",
    "                \"name\": \"system\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": messages_content,\n",
    "                \"role\": \"user\",\n",
    "                \"name\": \"user\"\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"mistral-7b-instruct\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"response_format\": {\n",
    "            \"type\": \"text\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Send POST request to the MaaS server\n",
    "    response = requests.post(maas_server_url, headers=headers, json=data)\n",
    "\n",
    "    # Process the MaaS response\n",
    "    try:\n",
    "        response_data = response.json()\n",
    "        if 'choices' in response_data:\n",
    "            result = response_data['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(\"Unexpected response structure:\", response_data)\n",
    "            result = \"Unexpected response format received from server. Check server logs for details.\"\n",
    "    except requests.exceptions.JSONDecodeError:\n",
    "        print(\"Failed to parse the response from the server:\", response.text)\n",
    "        result = \"Failed to parse the response from the server. Check server logs for more information.\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# Perform root cause analysis\n",
    "analysis = analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies)\n",
    "print(\"Root Cause Analysis:\")\n",
    "print(analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
