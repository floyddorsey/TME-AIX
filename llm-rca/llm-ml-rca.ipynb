{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM with RCA after anomaly detection using RAG<br>\n",
    "## Project Overview<br>\n",
    "Author: Sedat Kaymaz<br>\n",
    "This project aims to provide a root cause analsys method by using LLM with Dynamic RAG based on a system log file <br>\n",
    "for reference after applying anomaly detection ML method to the basic telecom metric list   <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once only\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Configuration\n",
    "def get_llm(model):\n",
    "    \"\"\"\n",
    "    Retrieves the Language Model (LLM) for root cause analysis.\n",
    "\n",
    "    This function retrieves the Language Model (LLM) used for root cause analysis. It first checks if the OpenAI API key is set as an environment variable. If not, it prompts the user to enter the API key and sets it as an environment variable. The function then returns an instance of the ChatOpenAI class with a temperature of 0 and the model name set to \"gpt-4\".\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: An instance of the ChatOpenAI class representing the Language Model (LLM) for root cause analysis.\n",
    "    \"\"\"\n",
    "    #If you are having issues with api key entry via embedded input, you can uncomment the line below and replace 'put_your_key_here' with your actual key\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = 'put_your_key_here'\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    print(openai_api_key)\n",
    "    if not openai_api_key:\n",
    "        openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    \n",
    "    # You can modify this to use different models or local LLMs\n",
    "    return ChatOpenAI(temperature=0, model_name=model)\n",
    "\n",
    "print(\"Loading Language Model...\")\n",
    "model='gpt-4'\n",
    "llm = get_llm(model)\n",
    "print(f\"Language Model {model} loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  call_attempt  call_success  call_failure  \\\n",
      "0 2024-09-04 00:00:00           114           110             0   \n",
      "1 2024-09-04 00:01:00           113           110             0   \n",
      "2 2024-09-04 00:02:00           114           111             0   \n",
      "3 2024-09-04 00:03:00           113           111             1   \n",
      "4 2024-09-04 00:04:00           112           111             1   \n",
      "\n",
      "   total_registered_subs  call_success_rate  \n",
      "0                   9031              96.40  \n",
      "1                   9084              97.34  \n",
      "2                   9089              97.36  \n",
      "3                   9035              98.23  \n",
      "4                   9092              99.10  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RAG for reading and processing metrics file\n",
    "def process_metrics_file(filename):\n",
    "    \"\"\"\n",
    "    Process the metrics file and return a vector store.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the metrics file.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore: A vector store containing the embeddings of the text documents.\n",
    "    \"\"\"\n",
    "    loader = CSVLoader(f\"data/{filename}\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Process metrics file\n",
    "metrics_vectorstore = process_metrics_file(\"metrics.csv\")\n",
    "\n",
    "# Load metrics data for anomaly detection\n",
    "df = pd.read_csv(\"data/metrics.csv\")\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies found:\n",
      "                   time  call_attempt  call_success  call_failure  \\\n",
      "683 2024-09-04 11:23:00           114            27             0   \n",
      "684 2024-09-04 11:24:00           113            32             0   \n",
      "685 2024-09-04 11:25:00           112            40             0   \n",
      "686 2024-09-04 11:26:00           114            70             2   \n",
      "\n",
      "     total_registered_subs  call_success_rate  is_anomaly  \n",
      "683                   9029             23.491          -1  \n",
      "684                   9038             28.368          -1  \n",
      "685                   9033             35.730          -1  \n",
      "686                   9157             61.491          -1  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anomaly detection using Isolation Forest\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"\n",
    "    Detects anomalies in the given DataFrame using the Isolation Forest algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame containing the data to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A subset of the input DataFrame containing only the rows that are classified as anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    features = ['call_attempt', 'call_success', 'call_failure', 'total_registered_subs', 'call_success_rate']\n",
    "    X = df[features]\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.005, random_state=42)\n",
    "    anomalies = iso_forest.fit_predict(X)\n",
    "    \n",
    "    df['is_anomaly'] = anomalies\n",
    "    return df[df['is_anomaly'] == -1]\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = detect_anomalies(df)\n",
    "if anomalies.empty:\n",
    "    print(\"No anomalies detected in the metrics.\")\n",
    "\n",
    "print(f\"Anomalies found:\\n{anomalies}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing log file...\n",
      "Log file has been processed.\n"
     ]
    }
   ],
   "source": [
    "# RAG for processing log file\n",
    "def process_log_file(filename):\n",
    "    \"\"\"\n",
    "    Process a log file and return a vector store.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the log file to process.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore: A vector store containing embeddings of the log file texts.\n",
    "    \"\"\"\n",
    "    loader = TextLoader(f\"data/{filename}\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# Process log file\n",
    "print(\"Processing log file...\")\n",
    "logs_vectorstore = process_log_file(\"systemd.log\")\n",
    "print(\"Log file has been processed.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Cause Analysis:\n",
      "Based on the provided metrics and system logs, the anomalies in the metrics seem to be related to the OpenStack services, specifically the Open vSwitch service. \n",
      "\n",
      "The metrics show a significant drop in call success rate starting from 2024-09-04 11:23:00. This coincides with the system logs which show that the Open vSwitch service encountered an error and crashed at 2024-09-04 11:22:13. The error message \"assertion pad_size <= dp_packet_size(b) failed in dp_packet_set_l2_pad_size()\" indicates a problem with packet padding size, which could potentially disrupt network traffic.\n",
      "\n",
      "The Open vSwitch service is a key component of the OpenStack platform, providing network connectivity for virtual machines. If this service fails, it could disrupt the network traffic, leading to call failures. \n",
      "\n",
      "The system logs also show that the Open vSwitch service was restarted at 2024-09-04 11:22:15, but the call success rate did not recover immediately, possibly due to ongoing network disruptions or other issues caused by the service crash.\n",
      "\n",
      "In addition, the logs show that there were several instances being migrated, rebooted, and created around the same time. These operations could also contribute to the network load and potentially exacerbate the impact of the Open vSwitch service crash.\n",
      "\n",
      "In conclusion, the root cause of the anomalies in the metrics is likely the crash of the Open vSwitch service, possibly exacerbated by high network load due to instance operations. Further investigation would be needed to determine why the Open vSwitch service crashed and how to prevent such issues in the future.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Root cause analysis using LLM\n",
    "def analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies):\n",
    "    \"\"\"\n",
    "    Analyzes the root cause of anomalies in the metrics based on the system logs.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM): The LLM object used for analysis.\n",
    "        metrics_vectorstore (VectorStore): The vector store containing metrics information.\n",
    "        logs_vectorstore (VectorStore): The vector store containing log information.\n",
    "        anomalies (list): A list of anomalies to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        str: A detailed root cause analysis for the given anomalies.\n",
    "\n",
    "    Example:\n",
    "        anomalies = [\"Anomaly1\", \"Anomaly2\"]\n",
    "        result = analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies)\n",
    "        print(result)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    Analyze the following anomalies in the metrics and provide a root cause analysis based on the system logs:\n",
    "\n",
    "    Anomalies:\n",
    "    {anomalies}\n",
    "\n",
    "    Relevant metrics information:\n",
    "    {metrics_info}\n",
    "\n",
    "    Relevant log information:\n",
    "    {logs_info}\n",
    "\n",
    "    Please provide a detailed root cause analysis for these anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"anomalies\", \"metrics_info\", \"logs_info\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    # Create a RunnableSequence\n",
    "    chain = (\n",
    "        {\n",
    "            \"anomalies\": RunnablePassthrough(),\n",
    "            \"metrics_info\": lambda x: metrics_vectorstore.similarity_search(str(x), k=2),\n",
    "            \"logs_info\": lambda x: logs_vectorstore.similarity_search(str(x), k=2)\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Invoke the chain\n",
    "    result = chain.invoke(anomalies)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Perform root cause analysis\n",
    "analysis = analyze_root_cause(llm, metrics_vectorstore, logs_vectorstore, anomalies)\n",
    "\n",
    "print(\"Root Cause Analysis:\")\n",
    "print(analysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
